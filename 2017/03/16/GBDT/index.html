<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>GBDT | Prestige</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文总结自知乎专栏文章：
Learn R | GBDT of Data Mining
是啥GBDT（Gradient Boosting Decision Tree）是一种基于迭代所构造的决策树算法。该算法生成多棵决策树，并将所有树的结果进行汇总来得出结果，将决策树和集成思想进行了有效的融合。
三个重点
GBDT中的DT是回归树而不是决策树
GB–梯度提升
GBDT中的shrinkage–缩减

G">
<meta property="og:type" content="article">
<meta property="og:title" content="GBDT">
<meta property="og:url" content="http://yoursite.com/2017/03/16/GBDT/index.html">
<meta property="og:site_name" content="Prestige">
<meta property="og:description" content="本文总结自知乎专栏文章：
Learn R | GBDT of Data Mining
是啥GBDT（Gradient Boosting Decision Tree）是一种基于迭代所构造的决策树算法。该算法生成多棵决策树，并将所有树的结果进行汇总来得出结果，将决策树和集成思想进行了有效的融合。
三个重点
GBDT中的DT是回归树而不是决策树
GB–梯度提升
GBDT中的shrinkage–缩减

G">
<meta property="og:updated_time" content="2017-03-16T08:59:32.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GBDT">
<meta name="twitter:description" content="本文总结自知乎专栏文章：
Learn R | GBDT of Data Mining
是啥GBDT（Gradient Boosting Decision Tree）是一种基于迭代所构造的决策树算法。该算法生成多棵决策树，并将所有树的结果进行汇总来得出结果，将决策树和集成思想进行了有效的融合。
三个重点
GBDT中的DT是回归树而不是决策树
GB–梯度提升
GBDT中的shrinkage–缩减

G">
  
    <link rel="alternate" href="/atom.xml" title="Prestige" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Prestige</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-GBDT" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/16/GBDT/" class="article-date">
  <time datetime="2017-03-16T08:58:20.000Z" itemprop="datePublished">2017-03-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      GBDT
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文总结自知乎专栏文章：</p>
<p><a href="https://zhuanlan.zhihu.com/p/25705586" target="_blank" rel="external">Learn R | GBDT of Data Mining</a></p>
<h3 id="是啥"><a href="#是啥" class="headerlink" title="是啥"></a>是啥</h3><p>GBDT（Gradient Boosting Decision Tree）是一种基于迭代所构造的决策树算法。该算法<strong>生成多棵决策树，并将所有树的结果进行汇总来得出结果</strong>，将决策树和集成思想进行了有效的融合。</p>
<h3 id="三个重点"><a href="#三个重点" class="headerlink" title="三个重点"></a>三个重点</h3><ol>
<li>GBDT中的DT是回归树而不是决策树</li>
<li>GB–梯度提升</li>
<li>GBDT中的shrinkage–缩减</li>
</ol>
<h4 id="GBDT之回归树"><a href="#GBDT之回归树" class="headerlink" title="GBDT之回归树"></a>GBDT之回归树</h4><p>GBDT中用的决策树并不是分类决策树，而是回归决策树。GBDT累加所有树的结果作为最终结果。</p>
<p>作为对比，简要回顾下分类树的运行过程：以ID3为例，穷举每一个属性特征的信息增益值，每一次都选取使信息增益最大的特征进行分枝，直到分类完成或达到预设的终止条件，实现决策树的递归构建。</p>
<p>回归树的运行流程与分类树基本类似，但有以下两点不同之处：</p>
<ul>
<li>第一，回归树的每个节点得到的是一个预测值而非分类树式的样本计数，假设在某一棵树的某一节点使用了年龄进行分枝（并假设在该节点上人数&gt;1），那么这个预测值就是属于这个节点的所有人年龄的平均值。</li>
<li>第二，在分枝节点的选取上，回归树并没有选用最大熵值来作为划分标准，而是使用了最小化均方差，即\(\frac{\sum_{i=1}^{n}{} (x_i-\bar{x} )^2}{n}\) 。这很好理解，被预测出错的次数越多，错的越离谱，均方差就越大，通过最小化均方差也就能够找到最靠谱的分枝依据。</li>
</ul>
<blockquote>
<p>一般来讲，回归树的分枝不太可能实现每个叶子节点上的属性值都唯一，更多的是达到我们预设的终止条件即可（例如叶子个数上限），这样势必会存在多个属性取值，那么该节点处的预测值自然就为基于这些样本所得到的平均值了。</p>
</blockquote>
<h4 id="GBDT之梯度提升"><a href="#GBDT之梯度提升" class="headerlink" title="GBDT之梯度提升"></a>GBDT之梯度提升</h4><p><strong>梯度提升是一种理念而非算法，其基本思想是沿着梯度方向训练一系列弱分类器并以一定的权重组合起来，形成最终的强分类器。</strong></p>
<p>这一系列弱分类器的训练方式便是GBDT的核心所在：<strong>每一棵树学习的是之前所有树结论和的残差</strong></p>
<blockquote>
<p>“Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？难道是每棵树独立训练一遍，比如A这个人，第一棵树认为是10岁，第二棵树认为是0岁，第三棵树认为是20岁，我们就取平均值10岁做最终结论？当然不是！且不说这是投票方法并不是GBDT，只要训练集不变，独立训练三次的三棵树必定完全相同，这样做完全没有意义。之前说过，GBDT是把所有树的结论累加起来做最终结论的，所以可以想到每棵树的结论并不是年龄本身，而是年龄的一个累加量。GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。”</p>
</blockquote>
<p>关于梯度提升的算法推导，请看李航老师《统计学习方法》中8.3节，提升树。</p>
<h4 id="GBDT之shrinkage"><a href="#GBDT之shrinkage" class="headerlink" title="GBDT之shrinkage"></a>GBDT之shrinkage</h4><blockquote>
<p>Shrinkage是GBDT的第三个基本概念，中文含义为“缩减”。它的基本思想就是：每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。换句话说缩减思想不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，只有通过多学几棵树才能弥补不足。</p>
</blockquote>
<p>Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001（注意该step非gradient的step），导致各个树的残差是渐变的而不是陡变的。直觉上这也很好理解，不像直接用残差一步修复误差，而是只修复一点点，其实就是把大步切成了很多小步。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。这个weight就是step。就像Adaboost一样，Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。</p>
<h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p><a href="http://blog.csdn.net/w28971023/article/details/8240756" target="_blank" rel="external">GBDT（MART） 迭代决策树入门教程 | 简介</a></p>
<p><a href="http://www.cnblogs.com/maybe2030/p/4734645.html" target="_blank" rel="external">决策树与迭代决策树（GBDT）</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/03/16/GBDT/" data-id="cj1iqqq1a000198r8ak7vp5px" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/20/蓄水池抽样/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          蓄水池抽样
        
      </div>
    </a>
  
  
    <a href="/2017/03/08/阿里巴巴阿里妈妈广告部门面试复盘/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">阿里巴巴阿里妈妈广告部门面试复盘</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试复盘/">面试复盘</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 16.67px;">Algorithm</a> <a href="/tags/Machine-Learning/" style="font-size: 13.33px;">Machine Learning</a> <a href="/tags/随笔/" style="font-size: 13.33px;">随笔</a> <a href="/tags/面试/" style="font-size: 10px;">面试</a> <a href="/tags/面试复盘/" style="font-size: 20px;">面试复盘</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/04/14/栈、队列、堆、优先队列的实现/">栈、队列、堆、优先队列的实现</a>
          </li>
        
          <li>
            <a href="/2017/04/12/Linked-List/">Linked List</a>
          </li>
        
          <li>
            <a href="/2017/04/12/K-NN/">K-NN</a>
          </li>
        
          <li>
            <a href="/2017/04/07/网易传媒技术部数据挖掘面试复盘/">网易传媒技术部数据挖掘面试复盘</a>
          </li>
        
          <li>
            <a href="/2017/03/23/百度系统部机器学习面试复盘/">百度系统部机器学习面试复盘</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Sammy Wang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>